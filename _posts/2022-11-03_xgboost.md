---
layout: single
title:  "XGBoost정리"
categories: machine_learning
tag: [machine_learning, XGBoost]
toc: True
---
# XGBboost

캐글에서 가장 사랑받고 많이 쓰이는 알고리즘 중에 하나인 xgboost를 정리 해보고자 한다.

xgboost는 기본적으로 GBM과 같이 decision Tree의 앙상블 모형이다. GBM을 더 발전시킨 

모형이며 비정형데이터에서는 Nerual Network 모델이 압도적인 성능을 보이지만 

정형데이터에서는 XGBoost와 같은 tree based 알고리즘이 가장 좋은 알고리즘으로 평가받는다.

Boosting이라는 앙상블 기법을 사용하는 XGBoost에서는 사용을한다.

앙상블이란 여러개의 모델을 사용해서 각각의 예측 결과를 만들고 그 예측 결과를 기반으로 최종

 예측결과를 정하는 앙상블 기법이며 비교적 약한 모델들을 결합하여 하나의 강한 모델을 만드는

 과정을 사용하는 Boosting, 그중 XGBoost는 Gradient Boosting알 사용하여 모델링을한다.

 GBM보다 XGBoost가 더 좋은 성능을 낼 수 있는 이유는 Optimization and alorithmic 

 즉, 최적화와 알고리즘입니다.

1. **System Optimization** : Parallelization(병렬 처리), Tree Pruning(가지치기) , 

   ​                                           Hardware Optimization(하드웨어 최적화)

2. **Algorithmic Enhancements** : Regularization, Sparsity Awareness,  Weighted Quantile Sketch,

   ​														 Cross-validation(Cross validation이 빌트인 되어있습니다.)



### 장점 : 계산 속도 향상, 학습 성능

sklearn에도 Gradient Boosting으로 Regression , Classification을 할 수 있는 매소드를 제공하고 있지만,

시간이 오래걸립니다. 하지만 XGBoost의 경우에는 병렬 연산을 지원하여 계산 속도를 향상 시킵니다.

또한 GPU를 사용 할 수 있는 옵션을 제공하기 때문에 학습하는데 소요되는 시간을 절약할 수 있습니다.

이것은 하이퍼파라미터 튜닝을 할 때도 특히 빛이 납니다.

학습 성능의 경우에는 kaggle에서 수차례 우승한 사례가 나왔기에 우수하다고 판단이 되어진다.



### 단점 : Tree Based Learning, 복잡한 하이퍼 파라미터

1. Tree Based Learning을 학습할 때 학습 데이터에 예민하게 반응합니다.

   학습 데이터에서 1~10 사이의 범주를 가진 라벨로 학습을 진행하는 모델이 있다면

   이 모델은 데이터 예측을 1 ~ 10 사이의 값을 예측하고자 하고 10초과, 1 미만으로 

   반환하기 어렵습니다.

2. 적절하게 튜닝을 하지 않은 모델은 Overfit이 쉽게 발생하는 문제 때문에 반드시

   징행해야하는 절차입니다. 그런데 XGBoost는 튜닝을 할 때 손봐야할 파라미터가

   너무 많습니다.

   XGBoost Parameters : https://xgboost.readthedocs.io/en/stable/parameter.html



### 3가지 구분에 따른 XGBoost 주요 하이퍼파라미터(파이썬 래퍼 기준)

##### 1. general parameter

        1) booster
           * gbtree 또는 gblinear 중 선택
           * Default = 'gbree' 
        2) slient 
           * 출력 메시지 설졍 관련 인수(나타내고 싶지 않을 경우 1로 설정)
           * Default  = 1
        3)  ntherad
           * CPU 실행 스레드 개수 조정
           * Default는 전체 다 사용하는 것
           * 멀티코어/스레드 CPU 시스템에서 일부 CPU만 사용할 때 변경



##### 2. booster parameter

​	1) eta(Default =0.3)
* 일반적으로 학습률로 불리는 파라미터
* weak learner의 반영 수준을 나타냄
* 범위는 0 ~ 1로 클 수록 모형의 업데이트 속도는 빨라짐
* 클수록 과적합 발생 가능성이 높아짐

​	2) num_boost_arond(Default =10)
* 학습에 활용될 weak larner의 반복 수

​	3) min_child_weight(Default =1)
* leaf node에 포함되는 최소 관측치의 수를 의미
* 작은 값을 가질수록 과적합 발생 가능성이 높음(과적합 조절 용도로 사용됨)
* 범위 : 0 ~ 무한대

​	4) gamma(Default =0)
* leaf node의 추가분할을 결정할 최소손실 감소값
* 해당값보다 손실이 크게 감소할 때 분리
* 값이 클수록 과적합 감소효과
* 범위 : 0 ~ 무한대

​	5) mex_depth(Default =6)

* 트리의 최대 깊이를 설정
* 0을 지정하면 깊이의 제한이 없음
* 과적합에 가장 민감하게 작용하는 파라미터 중 하나임
* 범위 : 0 ~ 무한대

​	6) sub_sample(Default =1)
* 학습시 데이터 샘플링 비율을 지정(과적합 제어)
* 일반적으로 0.5 ~ 1 사이의 값을 사용
* 범위 : 0 ~ 1

​	7) colsample_bytree(Default =1)
* 트리 생성에 필요한 feature의 샘플링에 사용
* feature가 많을 때 과적합 조절에 사용
* 범위 : 0 ~ 1

​	8) lambda(Default =1)
* L2 Regularization 적용 값
* featrue 개수가 많을 때 적용 검토
* 클수록 과적합 감소 효과

​	9) alpha(Default =0)

* L1 Regularization 적용 값
* featrue 개수가 많을 때 적용 검토
* 클수록 과적합 감소 효과

​	10) scale_pos_weight(Default =1)
* 불균형 데이터셋의 균형을 유지



##### 3. train parameter

​	1) objective
* reg:linear = 회귀
* binary:logistic = 이진분류
* multi:softmax = 다중분류, 클래스 변환
* multi:softprob = 다중분류, 확률반환

​	2) eval_metri
* r검증에 사용되는 함수정의
* 회구 분석인 겨우 'rmse'를, 클래스 분류 문제인 경우 'error'