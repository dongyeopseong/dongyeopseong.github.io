---
layout: single
title:  "분류회귀"
categories: machine_learning
tag: [machine_learning, 수학]
toc: True
---

# Regression vs Classification

****

### 회귀 (Regression) 

1. 입력값 : 연속값(실수형), 이산값(범주형) 등 모두 가능 
2. 출력값 : 연속값(실수형) 
3. 모델 형태 : 일반적인 함수 형태 (eg. y = w!x + w")



### 분류 (Classification)

1. 입력값 : 연속값(실수형), 이산값(범주형) 등 모두 가능 
2. 출력값 : 이산값(범주형) 
3. 모델 형태 : 이진 분류라면 시그모이드(sigmoid) 함수, 다중 분류라면 소프트맥스(softmax) 함수 꼭 포함



### 선형 회귀를 통한 분류

* 일반적인 회귀에서는 라벨의 순서(크기)에 따라 결과가 달라짐 • 

* 다른 손실 함수나 모델이 필요함



### 시그모이드 함수 (sigmoid function) 

* y = 1 / (1+e^-x)
*  x = 0 일때, 함수의 출력값은 0.5가 됨 
* 함수의 출력값이 항상 0 이상 1 이하임



### Logistic Regression

#### 오즈(odds)

* 성공(y=1)확률이 실패(y=0) 확률에 비해 몇 배 더 높은가를 나타냄
* odds = p(y=1|x) / 1-p(y=1|x)



### 로짓 변환(logit)

* 오즈에 로그를 취한 함수 형태
* 입력값(p)의 범위가 [0,1] 일 때, [-∞, +∞]를 출력함
* logit(p) = log(odds) = log * p(y=1|x) / 1-p(y=1|x)



### 로지스틱 함수(logistic function)

* 로짓 변환의 역함수로 해석가능
* 따라서 로지스틱 함수는 선형 회귀와 sigmoid 함수의 결합임